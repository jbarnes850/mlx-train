# ğŸš€ MLX Distributed Training Framework

Train and deploy AI models across multiple Apple Silicon devices with automatic hardware optimization and a seamless developer experience. A user-friendly CLI framework for distributed training, powered by MLX.

```bash
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘  __  __ _     __  __  _____           _                  â•‘
â•‘ |  \/  | |    \ \/ / |_   _| __ __ _ (_) _ __            â•‘
â•‘ | |\/| | |     \  /    | | | '__/ _` || || '_ \          â•‘
â•‘ | |  | | |___  /  \    | | | | | (_| || || | | |         â•‘
â•‘ |_|  |_|_____|/_/\_\   |_| |_|  \__,_||_||_| |_|         â•‘
â•‘                                                          â•‘
â•‘          Distributed Training on Apple Silicon           â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   MacBook Pro   â”‚  â†â†’  â”‚   MacBook Air   â”‚  Training
â”‚    Node 1       â”‚      â”‚    Node 2       â”‚  Cluster
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚    MLX Training      â”‚  Distributed
         â”‚   â•â•â•â•â•â•â•â•â•â•â• 100%   â”‚  Progress
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â–¼
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚  ğŸš€ Trained Model    â”‚  Local
         â”‚  localhost:8000      â”‚  Deployment
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

[![PyPI version](https://badge.fury.io/py/mlx-train.svg)](https://badge.fury.io/py/mlx-train)
[![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)](tests/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## âš¡ï¸ Quick Start & Interactive Script

```bash
# Clone and start
git clone https://github.com/jbarnes850/mlx-train
cd mlx-train

# Run the interactive training script
./scripts/mlx-train.sh
```

That's it! The script will:

- ğŸ› ï¸ Set up your environment automatically
- ğŸ” Detect your Apple Silicon devices
- ğŸ¤ Help connect multiple devices if available
- ğŸš€ Guide you through model training
- ğŸŒ Deploy your model locally

Watch your model train with real-time visualizations:

```bash
Device Utilization:
Device 1: [====================] 80.5% (12.8GB)
Device 2: [===================] 75.2% (12.1GB)

Training Progress:
[====================] 50%

Metrics:
â€¢ Throughput: 1250.32 tokens/sec
â€¢ Loss: 0.0234
â€¢ Memory Usage: 25.6GB
```

## âœ¨ Features

- ğŸš„ **Distributed Training**: Seamlessly scale across multiple Apple Silicon devices
- ğŸ”§ **Hardware Optimization**: Automatic detection and configuration for optimal performance
- ğŸ¯ **Zero-Config Setup**: Automatic environment setup and dependency management
- ğŸ“Š **Training Visualization**: Real-time metrics and progress tracking
- ğŸ§  **Model Development**: Build custom architectures or use pre-built components
- ğŸ”„ **Export & Serve**: Deploy models locally or export for other platforms

**Scale When Ready**:

- ğŸ”„ Start with any Apple Silicon Device (M1/M2/M3/M4)
- ğŸ”„ Add more devices anytime
- ğŸ”„ Automatic distributed training
- ğŸ”„ No code changes needed

## ğŸ›  Installation & Requirements

### Requirements

- Python 3.10 or higher
- Apple Silicon Mac (M1/M2/M3)
- 8GB RAM minimum (16GB+ recommended)
- macOS 12 or higher

### Quick Install

```bash

pip install mlx-train
```

### Development Setup

```bash
# Clone repository
git clone https://github.com/jbarnes850/mlx-train
cd mlx-train

# Create virtual environment
python3 -m venv venv
source venv/bin/activate

# Install dependencies
pip install -e ".[dev]"
```

The `mlx-train.sh` script will automatically check and install all requirements.

## ğŸ”§ Advanced Configuration

### Hardware Optimization

- Automatic device discovery
- Memory-aware batch size selection
- Gradient accumulation optimization

### Training Options

- Mixed precision training
- Gradient checkpointing
- Custom learning rate schedules

## ğŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

1. Fork the repository
2. Create your feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“š Documentation

[Documentation](docs/README.md)
[Installation](docs/installation.md)
[Examples](docs/examples.md)
[Core Concepts](docs/core.md)

## ğŸ”¬ Examples

Check out our [examples directory](examples/) for:

- Custom model training
- Multi-device distributed training
- Dataset preprocessing
- Model export and serving

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ“« Support

- ğŸ› [Issue Tracker](https://github.com/jbarnes850/mlx-train/issues)

---
Made with â¤ï¸ for the MLX community
